# 自动求导和实现方法
## 理论

## Torch实现
对y=2(xT)x求导
```
import torch
x=torch.arange(20)
x.requires_grad_(True) //表示记录x的梯度
y=2*torch.dot(x,x)
y.backward()
x.grad==4*x //check the result

x.grand.zero_()  //clear grad
y=x.sum()
y.backward()
x.grad

x.grad.zero_()
y=x*x
y.sum().backward()
x.grad  

x.grad.zero_()
y=x*x
u=y.detach()
z=u*x
z.sum().backward()
x.grand==u 
```

## 引入控制流的梯度计算
```
def f(a):
    b=a*2
    while b.norm()<1000:
        b=b*2
    if b.sum()>0:
        c=b
    else:
        c=100*b
    return c
a=torch.randn(size=(),requires_grad=True)
d=f(a)
d.backward()
a.grad==d/a    
```

## 总结
使用requires_grand记录梯度  
反向传递算出梯度  
调用a.grad查看梯度  